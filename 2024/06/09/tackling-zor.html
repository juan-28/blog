<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Cracking the XOR: Testing My Neural Network Library | Pranav’s Data Hub</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Cracking the XOR: Testing My Neural Network Library" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="One of the fundamental tests for any neural network implementation is the XOR problem. The XOR function is a classic example that demonstrates the limitations of simple linear models, as it cannot be learned with a single linear layer. By solving the XOR problem, I can validate the effectiveness of my neural network library and ensure that it can handle non-linear decision boundaries." />
<meta property="og:description" content="One of the fundamental tests for any neural network implementation is the XOR problem. The XOR function is a classic example that demonstrates the limitations of simple linear models, as it cannot be learned with a single linear layer. By solving the XOR problem, I can validate the effectiveness of my neural network library and ensure that it can handle non-linear decision boundaries." />
<link rel="canonical" href="https://juan-28.github.io/blog/2024/06/09/tackling-zor.html" />
<meta property="og:url" content="https://juan-28.github.io/blog/2024/06/09/tackling-zor.html" />
<meta property="og:site_name" content="Pranav’s Data Hub" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-09T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Cracking the XOR: Testing My Neural Network Library" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-09T00:00:00+00:00","datePublished":"2024-06-09T00:00:00+00:00","description":"One of the fundamental tests for any neural network implementation is the XOR problem. The XOR function is a classic example that demonstrates the limitations of simple linear models, as it cannot be learned with a single linear layer. By solving the XOR problem, I can validate the effectiveness of my neural network library and ensure that it can handle non-linear decision boundaries.","headline":"Cracking the XOR: Testing My Neural Network Library","mainEntityOfPage":{"@type":"WebPage","@id":"https://juan-28.github.io/blog/2024/06/09/tackling-zor.html"},"url":"https://juan-28.github.io/blog/2024/06/09/tackling-zor.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://juan-28.github.io/blog/feed.xml" title="Pranav&apos;s Data Hub" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Pranav&#39;s Data Hub</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Cracking the XOR: Testing My Neural Network Library</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-06-09T00:00:00+00:00" itemprop="datePublished">Jun 9, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>One of the fundamental tests for any neural network implementation is the XOR problem. The XOR function is a classic example that demonstrates the limitations of simple linear models, as it cannot be learned with a single linear layer. By solving the XOR problem, I can validate the effectiveness of my neural network library and ensure that it can handle non-linear decision boundaries.</p>

<h3 id="the-xor-problem">The XOR Problem</h3>

<p>The XOR function takes two binary inputs and outputs a binary value according to the following truth table:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input   | Output
--------|--------
0   0   |  1   0
1   0   |  0   1
0   1   |  0   1
1   1   |  1   0
</code></pre></div></div>

<p>This non-linear function cannot be solved by a single linear model, making it a perfect candidate to test the capabilities of a neural network.</p>

<h3 id="setting-up-the-experiment">Setting Up the Experiment</h3>

<p>To solve the XOR problem, I set up a neural network with the following architecture:</p>
<ul>
  <li><strong>Input Layer:</strong> 2 neurons (corresponding to the two inputs)</li>
  <li><strong>Hidden Layer:</strong> 2 neurons with Tanh activation</li>
  <li><strong>Output Layer:</strong> 2 neurons (corresponding to the two outputs)</li>
</ul>

<h3 id="code-implementation">Code Implementation</h3>

<p>Here’s the code I used to train the neural network on the XOR problem:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">nnet.train</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">nnet.nn</span> <span class="kn">import</span> <span class="n">NeuralNet</span>
<span class="kn">from</span> <span class="nn">nnet.layers</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Tanh</span>

<span class="c1"># Define the inputs and corresponding targets
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Initialize the neural network with a hidden layer and output layer
</span><span class="n">net</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">([</span>
    <span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">Tanh</span><span class="p">(),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Train the neural network on the XOR data
</span><span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># Test the trained network and print the results
</span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="results">Results</h3>

<p>After training the neural network, I obtained the following output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0 0] [ 9.99999996e-01 -4.23385083e-09] [1 0]
[1 0] [3.72591513e-09 1.00000000e+00] [0 1]
[0 1] [6.66842143e-09 1.00000001e+00] [0 1]
[1 1] [ 9.99999996e-01 -4.29201630e-09] [1 0]
</code></pre></div></div>

<h3 id="interpreting-the-results">Interpreting the Results</h3>

<p>The predicted outputs are close to the expected targets:</p>

<ul>
  <li>For input <code class="language-plaintext highlighter-rouge">[0, 0]</code>, the predicted output is close to <code class="language-plaintext highlighter-rouge">[1, 0]</code>.</li>
  <li>For input <code class="language-plaintext highlighter-rouge">[1, 0]</code>, the predicted output is close to <code class="language-plaintext highlighter-rouge">[0, 1]</code>.</li>
  <li>For input <code class="language-plaintext highlighter-rouge">[0, 1]</code>, the predicted output is close to <code class="language-plaintext highlighter-rouge">[0, 1]</code>.</li>
  <li>For input <code class="language-plaintext highlighter-rouge">[1, 1]</code>, the predicted output is close to <code class="language-plaintext highlighter-rouge">[1, 0]</code>.</li>
</ul>

<p>Although the predictions are not exact (due to the small discrepancies from numerical precision), they are sufficiently close to demonstrate that the neural network has successfully learned the XOR function. The near-zero values indicate high confidence in the predictions.</p>

<h3 id="thoughts">Thoughts</h3>

<p>Solving the XOR problem with my neural network library was a critical step in validating its functionality. The successful learning of the XOR function, a classic non-linear problem, indicates that this library can handle more complex tasks! This experiment solidified my understanding of neural networks and reinforced the robustness of my implementation.</p>

<p>Next steps include further testing with more complex datasets and exploring additional features to enhance the library’s capabilities. Maybe MNNIST? Stay tuned for more updates as I continue to expand and refine my neural network library!</p>

  </div><a class="u-url" href="/blog/2024/06/09/tackling-zor.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Pranav&#39;s Data Hub</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Pranav&#39;s Data Hub</li><li><a class="u-email" href="mailto:pranavsukumaran6@gmail.com">pranavsukumaran6@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/juan-28"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">juan-28</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Aspiring Data Professional and Lifelong Learner</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
